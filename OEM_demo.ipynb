{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTGkOTdOpBll"
   },
   "source": [
    "# OpenEarhMap Semantinc Segmentation\n",
    "\n",
    "This demo code demonstrates training and testing of UNet-EfficientNet-B4 for the OpenEarthMap dataset (https://open-earth-map.org/). This demo code is based on the work from the \"segmentation_models.pytorch\" repository by qubvel, available at: https://github.com/qubvel/segmentation_models.pytorch. We extend our sincere appreciation to the original author for their invaluable contributions to the field of semantic segmentation and for providing this open-source implementation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWiUctcOpBlo"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQdFTlpypBlp",
    "outputId": "755dab4f-711e-4438-c42c-a1a5cbbeb130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: rasterio in /opt/conda/lib/python3.11/site-packages (1.4.3)\n",
      "Requirement already satisfied: affine in /opt/conda/lib/python3.11/site-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.11/site-packages (from rasterio) (23.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from rasterio) (2024.7.4)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.11/site-packages (from rasterio) (8.1.7)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.11/site-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/conda/lib/python3.11/site-packages (from rasterio) (1.26.4)\n",
      "Requirement already satisfied: click-plugins in /opt/conda/lib/python3.11/site-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from rasterio) (3.1.2)\n",
      "Requirement already satisfied: pretrainedmodels in /opt/conda/lib/python3.11/site-packages (0.7.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from pretrainedmodels) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from pretrainedmodels) (0.18.1+cu121)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.11/site-packages (from pretrainedmodels) (4.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from pretrainedmodels) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->pretrainedmodels) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->pretrainedmodels) (12.5.82)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision->pretrainedmodels) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->pretrainedmodels) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->pretrainedmodels) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->pretrainedmodels) (1.3.0)\n",
      "Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from efficientnet_pytorch) (2.3.1+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet_pytorch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet_pytorch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.11/site-packages (1.0.14)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from timm) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from timm) (0.18.1+cu121)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (from timm) (0.27.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from timm) (0.5.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->timm) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.82)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: albumentations in /opt/conda/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.11/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (1.14.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /opt/conda/lib/python3.11/site-packages (from albumentations) (2.10.5)\n",
      "Requirement already satisfied: albucore==0.0.23 in /opt/conda/lib/python3.11/site-packages (from albumentations) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/conda/lib/python3.11/site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /opt/conda/lib/python3.11/site-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /opt/conda/lib/python3.11/site-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
      "Requirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: efficientnet-pytorch>=0.6.1 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (10.4.0)\n",
      "Requirement already satisfied: pretrainedmodels>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (1.16.0)\n",
      "Requirement already satisfied: timm>=0.9 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (1.0.14)\n",
      "Requirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.18.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.11/site-packages (from pretrainedmodels>=0.7.1->segmentation_models_pytorch) (4.0.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from timm>=0.9->segmentation_models_pytorch) (0.5.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8->segmentation_models_pytorch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->segmentation_models_pytorch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install rasterio\n",
    "!pip install pretrainedmodels\n",
    "!pip install efficientnet_pytorch\n",
    "!pip install timm\n",
    "!pip install albumentations\n",
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4elx5iipBlq"
   },
   "source": [
    "### Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zhMDpFqZpBlr"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "import source\n",
    "import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os768d4EpBlr"
   },
   "source": [
    "### Define main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOj5_N4apBlr",
    "outputId": "fcfc3a93-3030-4614-ecdf-ba54ca866b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs   : 5\n",
      "Number of classes  : 9\n",
      "Batch size         : 4\n",
      "Device             : cuda\n"
     ]
    }
   ],
   "source": [
    "OEM_ROOT = \"./demo/\"\n",
    "OEM_DATA_DIR = \"OpenEarthMap/\"\n",
    "TRAIN_LIST = OEM_DATA_DIR+\"train.txt\"\n",
    "VAL_LIST = OEM_DATA_DIR+\"val.txt\"\n",
    "TEST_LIST = OEM_DATA_DIR+\"test.txt\"\n",
    "WEIGHT_DIR = OEM_ROOT+\"weight\" # path to save weights\n",
    "OUT_DIR = OEM_ROOT+\"result/\" # path to save prediction images\n",
    "os.makedirs(WEIGHT_DIR, exist_ok=True)\n",
    "\n",
    "seed = 0\n",
    "learning_rate = 0.0001\n",
    "batch_size = 4\n",
    "n_epochs = 5\n",
    "classes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "n_classes = len(classes)+1\n",
    "classes_wt = np.ones([n_classes], dtype=np.float32)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Number of epochs   :\", n_epochs)\n",
    "print(\"Number of classes  :\", n_classes)\n",
    "print(\"Batch size         :\", batch_size)\n",
    "print(\"Device             :\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCETH4-PpBls"
   },
   "source": [
    "### Prepare training and validation file lists\n",
    "\n",
    "In this demo for Google Colab, we use only two regions, i.e., Tokyo and Kyoto for training. To train with the full set, please download the OpenEarthMap dataset from https://zenodo.org/record/7223446. Note for xBD data preparation is available at https://github.com/bao18/open_earth_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gn8aGxUvpBls",
    "outputId": "82f2abed-3597-4e31-abbf-a0c0c3d40804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples      : 3501\n",
      "Training samples   : 3000\n",
      "Validation samples : 500\n"
     ]
    }
   ],
   "source": [
    "img_pths = [f for f in Path(OEM_DATA_DIR).rglob(\"*.tif\") if \"/labels/\" in str(f)]\n",
    "\n",
    "train_pths = [str(f) for f in img_pths if f.name in np.loadtxt(TRAIN_LIST, dtype=str)]\n",
    "val_pths = [str(f) for f in img_pths if f.name in np.loadtxt(VAL_LIST, dtype=str)]\n",
    "#test_pths = [str(f) for f in img_pths if f.name in np.loadtxt(TEST_LIST, dtype=str)]\n",
    "\n",
    "print(\"Total samples      :\", len(img_pths))\n",
    "print(\"Training samples   :\", len(train_pths))\n",
    "print(\"Validation samples :\", len(val_pths))\n",
    "#print(\"Test samples       :\", len(test_pths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-uFSOcgpBlt"
   },
   "source": [
    "### Define training and validation dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenEarthMap/aachen/labels/aachen_11.tif'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DkjJGjCOpBlt"
   },
   "outputs": [],
   "source": [
    "trainset = source.dataset.Dataset(train_pths, classes=classes, size=512, train=True)\n",
    "validset = source.dataset.Dataset(val_pths, classes=classes, train=False)\n",
    "#testset = source.dataset.Dataset(test_pths, classes=classes, train=False)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "#test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR9cFGaspBlt"
   },
   "source": [
    "### Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTIQto7LpBlt",
    "outputId": "dceb44e7-4f97-4865-c153-aa65fe099f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output name  : u-efficientnet-b4_s0_CELoss\n",
      "Number of parameters:  20304278\n"
     ]
    }
   ],
   "source": [
    "network = smp.Unet(\n",
    "    classes=n_classes,\n",
    "    activation=None,\n",
    "    encoder_weights=\"imagenet\",\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    decoder_attention_type=\"scse\",\n",
    ")\n",
    "\n",
    "# count parameters\n",
    "params = 0\n",
    "for p in network.parameters():\n",
    "    if p.requires_grad:\n",
    "        params += p.numel()\n",
    "\n",
    "criterion = source.losses.CEWithLogitsLoss(weights=classes_wt)\n",
    "criterion_name = 'CE'\n",
    "metric = source.metrics.IoU2()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "network_fout = f\"{network.name}_s{seed}_{criterion.name}\"\n",
    "OUT_DIR += network_fout # path to save prediction images\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Model output name  :\", network_fout)\n",
    "print(\"Number of parameters: \", params)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Number of GPUs :\", torch.cuda.device_count())\n",
    "    network = torch.nn.DataParallel(network)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [dict(params=network.module.parameters(), lr=learning_rate)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2lQx068bQwo"
   },
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yTJFfnmspBlu"
   },
   "outputs": [],
   "source": [
    "class_rgb = {\n",
    "    \"Bareland\": [128, 0, 0],\n",
    "    \"Grass\": [0, 255, 36],\n",
    "    \"Pavement\": [148, 148, 148],\n",
    "    \"Road\": [255, 255, 255],\n",
    "    \"Tree\": [34, 97, 38],\n",
    "    \"Water\": [0, 69, 255],\n",
    "    \"Cropland\": [75, 181, 73],\n",
    "    \"buildings\": [222, 31, 7],\n",
    "}\n",
    "\n",
    "class_gray = {\n",
    "    \"Bareland\": 1,\n",
    "    \"Grass\": 2,\n",
    "    \"Pavement\": 3,\n",
    "    \"Road\": 4,\n",
    "    \"Tree\": 5,\n",
    "    \"Water\": 6,\n",
    "    \"Cropland\": 7,\n",
    "    \"buildings\": 8,\n",
    "}\n",
    "\n",
    "def label2rgb(a):\n",
    "    \"\"\"\n",
    "    a: labels (HxW)\n",
    "    \"\"\"\n",
    "    out = np.zeros(shape=a.shape + (3,), dtype=\"uint8\")\n",
    "    for k, v in class_gray.items():\n",
    "        out[a == v, 0] = class_rgb[k][0]\n",
    "        out[a == v, 1] = class_rgb[k][1]\n",
    "        out[a == v, 2] = class_rgb[k][2]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn9_yGisbYAN"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1t5R7gDKpBlu",
    "outputId": "6a0789b0-3d62-47fa-953d-1b700d006045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 750/750 [47:28<00:00,  3.80s/it, CELoss=1.54, mIoU=0.244] \n",
      "Valid: 100%|██████████| 125/125 [12:15<00:00,  5.88s/it, CELoss=1.11, mIoU=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 750/750 [46:12<00:00,  3.70s/it, CELoss=1.14, mIoU=0.369]\n",
      "Valid: 100%|██████████| 125/125 [09:11<00:00,  4.41s/it, CELoss=0.912, mIoU=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 750/750 [45:46<00:00,  3.66s/it, CELoss=1.01, mIoU=0.4]  \n",
      "Valid: 100%|██████████| 125/125 [08:46<00:00,  4.21s/it, CELoss=0.836, mIoU=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 750/750 [47:03<00:00,  3.76s/it, CELoss=0.919, mIoU=0.425]\n",
      "Valid: 100%|██████████| 125/125 [09:02<00:00,  4.34s/it, CELoss=0.815, mIoU=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 750/750 [45:57<00:00,  3.68s/it, CELoss=0.863, mIoU=0.462]\n",
      "Valid: 100%|██████████| 125/125 [09:14<00:00,  4.43s/it, CELoss=0.772, mIoU=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Processing time: 16860.144696712494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "max_score = 0\n",
    "train_hist = []\n",
    "valid_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  print(f\"\\nEpoch: {epoch + 1}\")\n",
    "\n",
    "  logs_train = source.runner.train_epoch(\n",
    "      model=network,\n",
    "      optimizer=optimizer,\n",
    "      criterion=criterion,\n",
    "      metric=metric,\n",
    "      dataloader=train_loader,\n",
    "      device=device,\n",
    "  )\n",
    "\n",
    "  logs_valid = source.runner.valid_epoch(\n",
    "      model=network,\n",
    "      criterion=criterion,\n",
    "      metric=metric,\n",
    "      dataloader=valid_loader,\n",
    "      device=device,\n",
    "  )\n",
    "\n",
    "  train_hist.append(logs_train)\n",
    "  valid_hist.append(logs_valid)\n",
    "\n",
    "  score = logs_valid[metric.name]\n",
    "\n",
    "  if max_score < score:\n",
    "      max_score = score\n",
    "      torch.save(network.state_dict(), os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\"))\n",
    "      print(\"Model saved!\")\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time:',end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONWiNQMWAHP4"
   },
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zip_files(output_dir, zip_filename):\n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        for root, _, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, output_dir))\n",
    "                os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "for fn_img in test_pths:\n",
    "    # Check if the image has already been processed\n",
    "    filename = os.path.splitext(os.path.basename(fn_img))[0]\n",
    "    output_path = os.path.join(OUT_DIR, f\"{filename}.png\")\n",
    "    if os.path.exists(output_path):\n",
    "        continue\n",
    "\n",
    "    img = source.dataset.load_multiband(fn_img)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    black_image = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    Image.fromarray(black_image).save(output_path)\n",
    "zip_files(OUT_DIR, \"processed.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network\n",
    "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\")))\n",
    "network.to(device).eval()\n",
    "\n",
    "test_pths = glob.glob(\"test/images/*.tif\")\n",
    "\n",
    "for fn_img in test_pths:\n",
    "  img = source.dataset.load_multiband(fn_img)\n",
    "  h, w = img.shape[:2]\n",
    "  power = math.ceil(np.log2(h) / np.log2(2))\n",
    "  shape = (2 ** power, 2 ** power)\n",
    "  img = cv2.resize(img, shape)\n",
    "\n",
    "  # test time augmentation\n",
    "  imgs = []\n",
    "  imgs.append(img.copy())\n",
    "  imgs.append(img[:, ::-1, :].copy())\n",
    "  imgs.append(img[::-1, :, :].copy())\n",
    "  imgs.append(img[::-1, ::-1, :].copy())\n",
    "\n",
    "  input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
    "\n",
    "  pred = []\n",
    "  with torch.no_grad():\n",
    "      msk = network(input)\n",
    "      msk = torch.softmax(msk[:, :, ...], dim=1)\n",
    "      msk = msk.cpu().numpy()\n",
    "      pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
    "  pred = pred.argmax(axis=0).astype(\"uint8\")\n",
    "  size = pred.shape[0:]\n",
    "  y_pr = cv2.resize(pred, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "  # save image as png\n",
    "  filename = os.path.splitext(os.path.basename(fn_img))[0]\n",
    "  Image.fromarray(y_pr).save(os.path.join(OUT_DIR, filename+'.tif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred processing .ipynb_checkpoints: [Errno 21] Is a directory: '/home/jovyan/open-earth-map/test/labels/.ipynb_checkpoints'. Skipping.\n",
      "Dataset mIoU: 0.26474113901459717\n",
      "mIoU per image:\n",
      "- aachen_4.tif: 0.26405045335720734\n",
      "- aachen_5.tif: 0.29852855377949916\n",
      "- aachen_1.tif: 0.312184203275812\n",
      "- aachen_2.tif: 0.24481229375161148\n",
      "- aachen_6.tif: 0.20413019090885565\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_iou(gt_mask, pred_mask, num_classes):\n",
    "    \"\"\"Calculates Intersection over Union (IoU) for each class.\n",
    "\n",
    "    Args:\n",
    "        gt_mask: Ground truth segmentation mask (NumPy array).\n",
    "        pred_mask: Predicted segmentation mask (NumPy array).\n",
    "        num_classes: Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing IoU for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    iou_per_class = np.zeros(num_classes)\n",
    "    for class_id in range(num_classes):\n",
    "        intersection = np.sum((gt_mask == class_id) & (pred_mask == class_id))\n",
    "        union = np.sum((gt_mask == class_id) | (pred_mask == class_id))\n",
    "\n",
    "        if union == 0:  # Handle cases where the union is zero (prevent division by zero)\n",
    "            iou_per_class[class_id] = 0.0  # Or np.nan if you prefer\n",
    "        else:\n",
    "            iou_per_class[class_id] = intersection / union\n",
    "\n",
    "    return iou_per_class\n",
    "\n",
    "\n",
    "def calculate_miou(gt_mask, pred_mask, num_classes):\n",
    "    \"\"\"Calculates mean Intersection over Union (mIoU).\n",
    "\n",
    "    Args:\n",
    "        gt_mask: Ground truth segmentation mask (NumPy array).\n",
    "        pred_mask: Predicted segmentation mask (NumPy array).\n",
    "        num_classes: Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        The mIoU score.\n",
    "    \"\"\"\n",
    "    iou_per_class = calculate_iou(gt_mask, pred_mask, num_classes)\n",
    "    miou = np.nanmean(iou_per_class) #Handle nan values that could arise from classes not present\n",
    "\n",
    "    return miou\n",
    "\n",
    "\n",
    "def load_mask(image_path):\n",
    "    \"\"\"Loads a segmentation mask from an image file.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array representing the mask.\n",
    "    \"\"\"\n",
    "    mask = Image.open(image_path)\n",
    "    mask = np.array(mask)\n",
    "    return mask\n",
    "\n",
    "def calculate_miou_for_dataset(gt_dir, pred_dir, num_classes):\n",
    "    \"\"\"Calculates mIoU over a dataset of images.\n",
    "\n",
    "    Args:\n",
    "        gt_dir: Path to the directory containing ground truth masks.\n",
    "        pred_dir: Path to the directory containing predicted masks.\n",
    "        num_classes: Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        The mean mIoU over the dataset.\n",
    "        A dictionary containing the mIoU for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    image_names = set(os.listdir(gt_dir)) & set(os.listdir(pred_dir)) #Find common file names\n",
    "    miou_scores = []\n",
    "    image_miou_dict = {}\n",
    "\n",
    "    if not image_names:\n",
    "        raise ValueError(\"No matching files found in the ground truth and prediction directories.\")\n",
    "\n",
    "    for image_name in image_names:\n",
    "        gt_path = os.path.join(gt_dir, image_name)\n",
    "        pred_path = os.path.join(pred_dir, image_name)\n",
    "\n",
    "        try:\n",
    "            gt_mask = load_mask(gt_path)\n",
    "            pred_mask = load_mask(pred_path)\n",
    "\n",
    "            if gt_mask.shape != pred_mask.shape:\n",
    "                print(f\"Warning: Masks for {image_name} have different shapes. Skipping.\")\n",
    "                continue #skip this image\n",
    "\n",
    "            miou = calculate_miou(gt_mask, pred_mask, num_classes)\n",
    "            miou_scores.append(miou)\n",
    "            image_miou_dict[image_name] = miou\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found: {image_name}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred processing {image_name}: {e}. Skipping.\")\n",
    "\n",
    "    if not miou_scores: #if no valid images were processed\n",
    "      return 0.0, {}\n",
    "\n",
    "    dataset_miou = np.mean(miou_scores)\n",
    "    return dataset_miou, image_miou_dict\n",
    "\n",
    "# Example paths (replace with your actual paths)\n",
    "gt_dir = \"test/labels\"  # Directory containing multiple GT masks\n",
    "pred_dir = OUT_DIR    # Directory containing multiple predicted masks\n",
    "num_classes = 8\n",
    "\n",
    "try:\n",
    "    dataset_miou, image_miou_dict = calculate_miou_for_dataset(gt_dir, pred_dir, num_classes)\n",
    "\n",
    "    print(f\"Dataset mIoU: {dataset_miou}\")\n",
    "\n",
    "    if image_miou_dict: #Print individual miou only if dictionary is not empty\n",
    "      print(\"mIoU per image:\")\n",
    "      for image_name, miou in image_miou_dict.items():\n",
    "          print(f\"- {image_name}: {miou}\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3I0hKRyHMl3"
   },
   "source": [
    "### Testing a model for a large Geotiff image\n",
    "\n",
    "A sample image is provided by the Geospatial Information Authority of Japan at https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WguTdAFqHWAe",
    "outputId": "f33e8f67-8953-4fca-bc11-11b670003234"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load network\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(WEIGHT_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork_fout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      5\u001b[0m network\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m test_large \u001b[38;5;241m=\u001b[39m TEST_DIR\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m35_1_op_2023.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# load network\n",
    "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\")))\n",
    "network.to(device).eval()\n",
    "\n",
    "test_large = TEST_DIR+\"35_1_op_2023.jpg\"\n",
    "\n",
    "# process large Geotiff image\n",
    "img0 = source.dataset.load_multiband(test_large)\n",
    "\n",
    "# get crs and transform\n",
    "crs, trans = source.dataset.get_crs(test_large)\n",
    "\n",
    "if img0.shape[2] > 3:\n",
    "    img0 = img0[:, :, :3]\n",
    "width = img0.shape[1]\n",
    "band = img0.shape[2]\n",
    "\n",
    "patch_size = 512\n",
    "stride = 256\n",
    "C = int(np.ceil( (width - patch_size) / stride ) + 1)\n",
    "R = int(np.ceil( (height - patch_size) / stride ) + 1)\n",
    "\n",
    "# weight matrix B for avoiding boundaries of patches\n",
    "if patch_size > stride:\n",
    "    w = patch_size\n",
    "    s1 = stride\n",
    "    s2 = w - s1\n",
    "    d = 1/(1+s2)\n",
    "    B1 = np.ones((w,w))\n",
    "    B1[:,s1::] = np.dot(np.ones((w,1)),(-np.arange(1,s2+1)*d+1).reshape(1,s2))\n",
    "    B2 = np.flip(B1)\n",
    "    B3 = B1.T\n",
    "    B4 = np.flip(B3)\n",
    "    B = B1*B2*B3*B4\n",
    "else:\n",
    "    B = np.ones((w,w))\n",
    "\n",
    "img1 = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1),3))\n",
    "img1[0:height,0:width,:] = img0.copy()\n",
    "\n",
    "pred_all = np.zeros((9,patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
    "weight = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
    "\n",
    "for r in range(R):\n",
    "    for c in range(C):\n",
    "        img = img1[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size,:].copy().astype(np.float32)/255\n",
    "        imgs = []\n",
    "        imgs.append(img.copy())\n",
    "        imgs.append(img[:, ::-1, :].copy())\n",
    "        imgs.append(img[::-1, :, :].copy())\n",
    "        imgs.append(img[::-1, ::-1, :].copy())\n",
    "\n",
    "        input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
    "\n",
    "        pred = []\n",
    "        with torch.no_grad():\n",
    "            msk = network(input)\n",
    "            msk = torch.softmax(msk[:, :, ...], dim=1)\n",
    "            msk = msk.cpu().numpy()\n",
    "\n",
    "            pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
    "\n",
    "        pred_all[:,r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += pred.copy()*B\n",
    "        weight[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += B\n",
    "\n",
    "for b in range(9):\n",
    "    pred_all[b,:,:] = pred_all[b,:,:]/weight\n",
    "    if b == 0:\n",
    "        pred_all[b,:,:] = 0\n",
    "\n",
    "pred_all = pred_all.argmax(axis=0).astype(\"uint8\")\n",
    "\n",
    "filename = os.path.splitext(os.path.basename(test_large))[0]\n",
    "pr_rgb = label2rgb(pred_all)\n",
    "Image.fromarray(pr_rgb[0:height,0:width,:]).save(os.path.join(OUT_DIR, filename+'_pr.png'))\n",
    "\n",
    "# save geotiff\n",
    "pr_rgb = np.transpose(pr_rgb[0:height,0:width,:], (2,0,1))\n",
    "source.dataset.save_img(os.path.join(OUT_DIR, filename+'_pr.tif'),pr_rgb,crs,trans)\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time:',end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing patch  0 \n",
      "\n",
      "processing patch  8 \n",
      "\n",
      "processing patch  16 \n",
      "\n",
      "processing patch  24 \n",
      "\n",
      "processing patch  32 \n",
      "\n",
      "processing patch  40 \n",
      "\n",
      "processing patch  48 \n",
      "\n",
      "processing patch  56 \n",
      "\n",
      "processing patch  64 \n",
      "\n",
      "processing patch  72 \n",
      "\n",
      "processing patch  80 \n",
      "\n",
      "processing patch  88 \n",
      "\n",
      "processing patch  96 \n",
      "\n",
      "processing patch  104 \n",
      "\n",
      "processing patch  112 \n",
      "\n",
      "processing patch  120 \n",
      "\n",
      "processing patch  128 \n",
      "\n",
      "processing patch  136 \n",
      "\n",
      "processing patch  144 \n",
      "\n",
      "processing patch  152 \n",
      "\n",
      "processing patch  160 \n",
      "\n",
      "processing patch  168 \n",
      "\n",
      "processing patch  176 \n",
      "\n",
      "processing patch  184 \n",
      "\n",
      "processing patch  192 \n",
      "\n",
      "processing patch  200 \n",
      "\n",
      "processing patch  208 \n",
      "\n",
      "processing patch  216 \n",
      "\n",
      "processing patch  224 \n",
      "\n",
      "processing patch  232 \n",
      "\n",
      "processing patch  240 \n",
      "\n",
      "processing patch  248 \n",
      "\n",
      "processing patch  256 \n",
      "\n",
      "processing patch  264 \n",
      "\n",
      "processing patch  272 \n",
      "\n",
      "processing patch  280 \n",
      "\n",
      "processing patch  288 \n",
      "\n",
      "processing patch  296 \n",
      "\n",
      "processing patch  304 \n",
      "\n",
      "processing patch  312 \n",
      "\n",
      "processing patch  320 \n",
      "\n",
      "processing patch  328 \n",
      "\n",
      "processing patch  336 \n",
      "\n",
      "processing patch  344 \n",
      "\n",
      "processing patch  352 \n",
      "\n",
      "processing patch  360 \n",
      "\n",
      "processing patch  368 \n",
      "\n",
      "processing patch  376 \n",
      "\n",
      "processing patch  384 \n",
      "\n",
      "processing patch  392 \n",
      "\n",
      "processing patch  400 \n",
      "\n",
      "processing patch  408 \n",
      "\n",
      "processing patch  416 \n",
      "\n",
      "processing patch  424 \n",
      "\n",
      "processing patch  432 \n",
      "\n",
      "processing patch  440 \n",
      "\n",
      "processing patch  448 \n",
      "\n",
      "processing patch  456 \n",
      "\n",
      "processing patch  464 \n",
      "\n",
      "processing patch  472 \n",
      "\n",
      "processing patch  480 \n",
      "\n",
      "processing patch  488 \n",
      "\n",
      "processing patch  496 \n",
      "\n",
      "processing patch  504 \n",
      "\n",
      "processing patch  512 \n",
      "\n",
      "processing patch  520 \n",
      "\n",
      "processing patch  528 \n",
      "\n",
      "processing patch  536 \n",
      "\n",
      "processing patch  544 \n",
      "\n",
      "processing patch  552 \n",
      "\n",
      "processing patch  560 \n",
      "\n",
      "processing patch  568 \n",
      "\n",
      "processing patch  576 \n",
      "\n",
      "processing patch  584 \n",
      "\n",
      "processing patch  592 \n",
      "\n",
      "processing patch  600 \n",
      "\n",
      "processing patch  608 \n",
      "\n",
      "processing patch  616 \n",
      "\n",
      "processing patch  624 \n",
      "\n",
      "processing patch  632 \n",
      "\n",
      "processing patch  640 \n",
      "\n",
      "processing patch  648 \n",
      "\n",
      "processing patch  656 \n",
      "\n",
      "processing patch  664 \n",
      "\n",
      "processing patch  672 \n",
      "\n",
      "processing patch  680 \n",
      "\n",
      "processing patch  688 \n",
      "\n",
      "processing patch  696 \n",
      "\n",
      "processing patch  704 \n",
      "\n",
      "processing patch  712 \n",
      "\n",
      "processing patch  720 \n",
      "\n",
      "processing patch  728 \n",
      "\n",
      "processing patch  736 \n",
      "\n",
      "processing patch  744 \n",
      "\n",
      "processing patch  752 \n",
      "\n",
      "processing patch  760 \n",
      "\n",
      "processing patch  768 \n",
      "\n",
      "processing patch  776 \n",
      "\n",
      "processing patch  784 \n",
      "\n",
      "processing patch  792 \n",
      "\n",
      "processing patch  800 \n",
      "\n",
      "processing patch  808 \n",
      "\n",
      "processing patch  816 \n",
      "\n",
      "processing patch  824 \n",
      "\n",
      "processing patch  832 \n",
      "\n",
      "processing patch  840 \n",
      "\n",
      "processing patch  848 \n",
      "\n",
      "processing patch  856 \n",
      "\n",
      "processing patch  864 \n",
      "\n",
      "processing patch  872 \n",
      "\n",
      "processing patch  880 \n",
      "\n",
      "processing patch  888 \n",
      "\n",
      "processing patch  896 \n",
      "\n",
      "processing patch  904 \n",
      "\n",
      "processing patch  912 \n",
      "\n",
      "processing patch  920 \n",
      "\n",
      "processing patch  928 \n",
      "\n",
      "processing patch  936 \n",
      "\n",
      "processing patch  944 \n",
      "\n",
      "processing patch  952 \n",
      "\n",
      "processing patch  960 \n",
      "\n",
      "processing patch  968 \n",
      "\n",
      "processing patch  976 \n",
      "\n",
      "processing patch  984 \n",
      "\n",
      "processing patch  992 \n",
      "\n",
      "processing patch  1000 \n",
      "\n",
      "processing patch  1008 \n",
      "\n",
      "processing patch  1016 \n",
      "\n",
      "processing patch  1024 \n",
      "\n",
      "processing patch  1032 \n",
      "\n",
      "processing patch  1040 \n",
      "\n",
      "processing patch  1048 \n",
      "\n",
      "processing patch  1056 \n",
      "\n",
      "processing patch  1064 \n",
      "\n",
      "processing patch  1072 \n",
      "\n",
      "processing patch  1080 \n",
      "\n",
      "processing patch  1088 \n",
      "\n",
      "Processing time: 740.0337600708008\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Load network\n",
    "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\")))\n",
    "network.to(device).eval()\n",
    "\n",
    "# Load large GeoTIFF image and metadata\n",
    "test_large = TEST_DIR + \"35_1_op_2023.jpg\"\n",
    "img0 = source.dataset.load_multiband(test_large)\n",
    "crs, trans = source.dataset.get_crs(test_large)\n",
    "\n",
    "# Ensure 3-band (RGB) data\n",
    "if img0.shape[2] > 3:\n",
    "    img0 = img0[:, :, :3]\n",
    "\n",
    "height, width, _ = img0.shape\n",
    "patch_size = 512\n",
    "\n",
    "# Padding image to ensure even division into patches\n",
    "pad_h = (patch_size - height % patch_size) % patch_size\n",
    "pad_w = (patch_size - width % patch_size) % patch_size\n",
    "img_padded = np.pad(img0, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "\n",
    "# Prepare output arrays\n",
    "pred_all = np.zeros((9, height + pad_h, width + pad_w), dtype=np.float32)\n",
    "\n",
    "# Divide into non-overlapping patches\n",
    "patches = [\n",
    "    img_padded[r:r+patch_size, c:c+patch_size].astype(np.float32) / 255\n",
    "    for r in range(0, img_padded.shape[0], patch_size)\n",
    "    for c in range(0, img_padded.shape[1], patch_size)\n",
    "]\n",
    "\n",
    "# Batch process patches\n",
    "batch_size = 8  # Adjust based on GPU memory\n",
    "for i in range(0, len(patches), batch_size):\n",
    "    print(\"processing patch \", i, \"\\n\")\n",
    "    batch_patches = patches[i:i+batch_size]\n",
    "    \n",
    "    # Test time augmentation\n",
    "    augmented_patches = []\n",
    "    for img in batch_patches:\n",
    "        augmented_patches.extend([\n",
    "            img.copy(),  # Original\n",
    "            img[:, ::-1, :].copy(),  # Flip horizontally\n",
    "            img[::-1, :, :].copy(),  # Flip vertically\n",
    "            img[::-1, ::-1, :].copy()  # Flip both axes\n",
    "        ])\n",
    "    \n",
    "    input_tensor = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in augmented_patches], dim=0).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        msk = network(input_tensor)\n",
    "        msk = torch.softmax(msk, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Aggregate predictions\n",
    "    for idx, img in enumerate(batch_patches):\n",
    "        r, c = divmod(i + idx, img_padded.shape[1] // patch_size)\n",
    "        pred = (\n",
    "            msk[idx*4] +\n",
    "            msk[idx*4+1][:, :, ::-1] +\n",
    "            msk[idx*4+2][:, ::-1, :] +\n",
    "            msk[idx*4+3][:, ::-1, ::-1]\n",
    "        ) / 4\n",
    "        \n",
    "        pred_all[:, r*patch_size:(r+1)*patch_size, c*patch_size:(c+1)*patch_size] = pred\n",
    "\n",
    "# Finalize predictions\n",
    "pred_all = pred_all.argmax(axis=0).astype(\"uint8\")[:height, :width]\n",
    "\n",
    "# Save outputs\n",
    "filename = os.path.splitext(os.path.basename(test_large))[0]\n",
    "pr_rgb = label2rgb(pred_all)\n",
    "Image.fromarray(pr_rgb).save(os.path.join(OUT_DIR, filename + '_pr.png'))\n",
    "\n",
    "# Save GeoTIFF\n",
    "# pr_rgb = np.transpose(pr_rgb, (2, 0, 1))\n",
    "# source.dataset.save_img(os.path.join(OUT_DIR, filename + '_pr.tif'), pr_rgb, crs, trans)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processing time:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8adc929056e836a7a3b0e312c82a4cd60bdf00e8c79486eca7d6ecc652c519a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
